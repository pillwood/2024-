{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36a2146",
   "metadata": {},
   "source": [
    "## 이름: 홍진우\n",
    "\n",
    "## 학번: 20205276"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c04dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural networks with PyTorch\n",
    "\n",
    "Pytorch의 `nn.module`을 활용하여 만드는 유용한 방법을 학습합니다.\n",
    "\n",
    "<div style=\"text-align:center\"><img src='https://drive.google.com/thumbnail?id=1J2SeiPpVJs1-ML2BdLrcxkGGmHpRxIVE&sz=w1000' width=\"250\" height=\"200\"> \n",
    "\n",
    "### Lego block coding! </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd06918",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2169f7d9850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605a354",
   "metadata": {},
   "source": [
    "`nn.Linear`: $Z^{[\\ell]} = A^{[\\ell-1]}W^T+b$\n",
    "연산.\n",
    "\n",
    "해당 layer의 \n",
    "\n",
    "- 입력 차원 `n_input=30`\n",
    "- 출력 차원 `n_output=60`\n",
    "\n",
    "\n",
    "\n",
    "In이 30, Out이 60인nn\n",
    "\n",
    "Column은 60\n",
    "\n",
    "Row는 batch수\n",
    "\n",
    "A는 30 W는 30을 60으로 바꿔줘야 하므로, 30 by 60\n",
    "\n",
    "해당 과정 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fe1c09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of nn.linear (In수, Out수, Batch는 알아서 됨. 어짜피 입력의 shape 0번이 batch사이즈)\n",
    "linear_layer1 = nn.Linear(30, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e4a14fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4969,  0.0564,  0.0403,  ...,  0.7190, -0.4373, -0.2725],\n",
       "        [ 0.1313, -0.3374, -0.4736,  ...,  0.9976, -0.5047,  0.2178],\n",
       "        [-0.1740, -1.1645, -0.4352,  ..., -0.0982, -0.6580,  0.6408],\n",
       "        ...,\n",
       "        [-0.3432,  0.3072,  0.7628,  ..., -0.3314,  0.2335,  0.1517],\n",
       "        [ 0.5272,  0.3454, -0.5879,  ...,  0.7543,  0.7225,  0.2116],\n",
       "        [ 0.3033, -0.1973,  0.3881,  ..., -0.1832,  1.0450,  0.7153]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test sample 생성해서 넣어줌\n",
    "A = torch.randn(60, 30)\n",
    "#60,30으로 넣어줘야 60 by 60나옴.. 곱해줄때, 전치되서 곱해짐\n",
    "linear_layer1(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff100f6-96f6-4215-baaa-ce6ec9d7c2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 60])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1(A).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e23309-e60b-47fc-b824-1b85735414a2",
   "metadata": {},
   "source": [
    "근데 W 랑 B는 어디서??\n",
    "\n",
    "linear_layer1에서 알아서 넣어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8b1d6a7-95e0-4c3d-9b1a-28f608e68a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1186, -0.1156, -0.0194,  ...,  0.0404,  0.0339,  0.1559],\n",
       "        [-0.1508,  0.1334,  0.0128,  ...,  0.1028,  0.0347, -0.0401],\n",
       "        [ 0.0765, -0.1189,  0.0270,  ..., -0.1132,  0.0881, -0.0964],\n",
       "        ...,\n",
       "        [-0.0902, -0.0866,  0.0548,  ...,  0.1206, -0.0104, -0.0793],\n",
       "        [-0.0640,  0.0198,  0.1421,  ...,  0.0484,  0.1701, -0.1360],\n",
       "        [ 0.1803,  0.0207, -0.1754,  ...,  0.0740, -0.1130, -0.0051]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bcbcbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "+ How to get the weights and bias of each `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e2e089b-cda3-4497-8e1f-03ced5829c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1186, -0.1156, -0.0194,  ...,  0.0404,  0.0339,  0.1559],\n",
       "        [-0.1508,  0.1334,  0.0128,  ...,  0.1028,  0.0347, -0.0401],\n",
       "        [ 0.0765, -0.1189,  0.0270,  ..., -0.1132,  0.0881, -0.0964],\n",
       "        ...,\n",
       "        [-0.0902, -0.0866,  0.0548,  ...,  0.1206, -0.0104, -0.0793],\n",
       "        [-0.0640,  0.0198,  0.1421,  ...,  0.0484,  0.1701, -0.1360],\n",
       "        [ 0.1803,  0.0207, -0.1754,  ...,  0.0740, -0.1130, -0.0051]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e636fd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of weights\n",
    "linear_layer1.weight.data = torch.ones_like(linear_layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "214864da-24f2-454f-8f90-3db57cfefa91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a96b2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NN example\n",
    "\n",
    "- input units: 20\n",
    "- hidden layer: 30, 40\n",
    "- output units: 3\n",
    "- activation function: ReLU\n",
    "- output layer: No activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c09e678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple NN construction\n",
    "\n",
    "# 클래스로 선언\n",
    "class FCN(nn.Module):#nn.Module 써야 함. => Base class for all NN modules.\n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()#안쓰면 오류남. python문법 상속 클래스의 인스턴스를 임시로 불러옴, 그리고 실행.\n",
    "        \n",
    "        self.lin1 = nn.Linear(20, 30) #Input Hidden\n",
    "        self.lin2 = nn.Linear(30, 40) #Hidden Hidden\n",
    "        self.lin3 = nn.Linear(40, 3) #Hidden Output\n",
    "        self.relu = nn.ReLU(True) #Use ReLU... 안해도 되는데 inplace연산해서 메모리 절약 가능. 디폴트는 False\n",
    "        \n",
    "    #forward의 장점 : model.함수명 안하고 그냥 model만 적어도 알아서 forward불러옴\n",
    "    def forward(self, x): #\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88009d27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0509, -0.0498, -0.0223],\n",
       "        [-0.1126,  0.0252,  0.0171],\n",
       "        [-0.0315,  0.0152, -0.0474],\n",
       "        [-0.0708, -0.0118, -0.0132],\n",
       "        [-0.0566, -0.0194, -0.0388],\n",
       "        [-0.1082,  0.0166, -0.0127],\n",
       "        [-0.0202,  0.0185, -0.1269],\n",
       "        [-0.1490, -0.0388, -0.0302],\n",
       "        [-0.0678,  0.0493, -0.0363],\n",
       "        [-0.0430, -0.0925, -0.0578],\n",
       "        [-0.0670,  0.0326,  0.0037],\n",
       "        [-0.1168, -0.0059, -0.0126],\n",
       "        [-0.1966,  0.0233, -0.1207],\n",
       "        [-0.0602,  0.0260, -0.0027],\n",
       "        [-0.1000, -0.0419, -0.0509],\n",
       "        [-0.1089, -0.0455, -0.0128],\n",
       "        [-0.1287,  0.0223, -0.1286],\n",
       "        [-0.0514,  0.0314,  0.0269],\n",
       "        [ 0.0050, -0.0032, -0.1564],\n",
       "        [-0.0479, -0.0459, -0.0974],\n",
       "        [-0.1007, -0.0102, -0.0348],\n",
       "        [-0.0536, -0.0622, -0.0419],\n",
       "        [-0.0968, -0.0488, -0.1594],\n",
       "        [-0.1790, -0.0703, -0.1127],\n",
       "        [-0.0734, -0.0862, -0.1104],\n",
       "        [-0.1266,  0.0027,  0.0104],\n",
       "        [-0.0818,  0.0135, -0.0297],\n",
       "        [-0.0564, -0.0486, -0.1428],\n",
       "        [-0.1159,  0.0079,  0.0511],\n",
       "        [-0.0646, -0.0879, -0.0810],\n",
       "        [-0.0445,  0.0384, -0.0885],\n",
       "        [-0.0592, -0.0019, -0.2207],\n",
       "        [-0.0379, -0.0818, -0.0812],\n",
       "        [-0.0975,  0.0190, -0.1170],\n",
       "        [-0.0965, -0.0159,  0.0163],\n",
       "        [-0.1255,  0.0122, -0.0168],\n",
       "        [-0.0865, -0.0125, -0.0029],\n",
       "        [-0.0711, -0.0252, -0.0335],\n",
       "        [-0.1213,  0.0297, -0.0354],\n",
       "        [-0.0928,  0.0579, -0.0230],\n",
       "        [-0.0704,  0.0158, -0.0184],\n",
       "        [-0.0976, -0.0594, -0.0387],\n",
       "        [-0.1226,  0.0351, -0.0145],\n",
       "        [-0.1082,  0.0172, -0.0317],\n",
       "        [-0.0278, -0.0097, -0.0637],\n",
       "        [-0.1180, -0.0061, -0.0303],\n",
       "        [-0.0297,  0.0130, -0.1037],\n",
       "        [-0.0673,  0.0493, -0.0712],\n",
       "        [-0.0744,  0.0072, -0.0227],\n",
       "        [-0.1160, -0.0119, -0.0116],\n",
       "        [-0.0966, -0.0282,  0.0039],\n",
       "        [-0.0358,  0.0259, -0.0112],\n",
       "        [-0.1024,  0.0284, -0.0540],\n",
       "        [-0.1131,  0.0757, -0.0643],\n",
       "        [-0.0845, -0.0197, -0.0235],\n",
       "        [-0.0283, -0.0179, -0.0389],\n",
       "        [-0.0402,  0.0668, -0.0753],\n",
       "        [-0.0730, -0.0180, -0.1112],\n",
       "        [-0.1138,  0.0046,  0.0240],\n",
       "        [-0.0756,  0.0065,  0.0585]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = torch.randn(60, 20)\n",
    "\n",
    "model = FCN()\n",
    "model(Xtrain)\n",
    "# = model.forward(Xtrain)\n",
    "\n",
    "#만약 특정 레이어의 weight를 바꾸고 싶다면? \n",
    "#model.lin1.weight.data = torch.ones(30,20)\n",
    "#model.lin1.weight.data = torch.ones_like(model.lin1.weight) 는 알아서 차원 맞춰주는데 숙제는 이거 쓰지 말고 직접"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f646b76f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (lin1): Linear(in_features=20, out_features=30, bias=True)\n",
       "  (lin2): Linear(in_features=30, out_features=40, bias=True)\n",
       "  (lin3): Linear(in_features=40, out_features=3, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a24fa36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of parameters() in models\n",
    "#파라메터 관련 내용..\n",
    "\n",
    "# param_iterator = model.parameters()\n",
    "\n",
    "# for param in param_iterator:\n",
    "#     print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "929f4cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nn.Sequential() example\n",
    "\n",
    "class FCN_seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(20, 30),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Linear(30, 40),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Linear(40, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "653f0bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_seq(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq = FCN_seq()\n",
    "Xtrain.shape\n",
    "model_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0db292b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=30, bias=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq.fc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a16dc4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FCN_seq_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        \n",
    "        temp = self.fcn_block(20, 30)+self.fcn_block(30, 40)+[nn.Linear(40,1)]\n",
    "        self.fc = nn.Sequential(*temp)\n",
    "        \n",
    "        \n",
    "    def fcn_block(self, in_dim, out_dim):\n",
    "        return [nn.Linear(in_dim, out_dim),\n",
    "                             nn.ReLU(True)]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ac30117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_seq_v2(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=40, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq_v2 = FCN_seq_v2()\n",
    "model_seq_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "514cccdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FCN_final(nn.Module):\n",
    "    def __init__(self, in_dim, hlayer, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        l_list = self.fcn_block(in_dim, hlayer[0])\n",
    "        \n",
    "        for l1, l2 in zip(hlayer[:-1], hlayer[1:]):\n",
    "            l_list = l_list + self.fcn_block(l1, l2)\n",
    "        \n",
    "        l_list = l_list + [nn.Linear(hlayer[-1], out_dim)]\n",
    "        \n",
    "        self.fc = nn.Sequential(*l_list)\n",
    "        \n",
    "        \n",
    "    def fcn_block(self, in_dim, out_dim):\n",
    "        return [nn.Linear(in_dim, out_dim),\n",
    "                             nn.ReLU(True)]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05a543cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hlayer = [30, 40]\n",
    "in_dim = 20\n",
    "out_dim= 3\n",
    "\n",
    "myfcn_final = FCN_final(in_dim, hlayer, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d545559c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_final(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfcn_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d419c48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ordered dict example\n",
    "# nn.Sequential() example\n",
    "\n",
    "class FCN_seq_ordered_dic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(OrderedDict([('lin1', nn.Linear(20, 30)),\n",
    "                      ('relu1', nn.ReLU(True)),\n",
    "                      ('lin2', nn.Linear(30, 40)),\n",
    "                      ('relu2',nn.ReLU(True)),\n",
    "                      ('lin3', nn.Linear(40, 3))\n",
    "                                            ])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a876e852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ModuleList(), ModuleDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb4c2ad8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lin1.weight',\n",
       "              tensor([[ 0.0299,  0.1571,  0.0047,  0.0533, -0.0884, -0.2145,  0.0765,  0.0220,\n",
       "                       -0.0863,  0.1192,  0.1070,  0.0295,  0.1065, -0.0805,  0.0646,  0.1734,\n",
       "                        0.0843,  0.0162,  0.0501, -0.0247],\n",
       "                      [-0.1972, -0.1207,  0.1918, -0.0233,  0.2191, -0.0912,  0.1933,  0.1021,\n",
       "                        0.1646,  0.2095, -0.0597, -0.0218,  0.1179,  0.0465, -0.1164, -0.1304,\n",
       "                       -0.0958, -0.1624, -0.1906, -0.0230],\n",
       "                      [-0.0408,  0.1159, -0.1523, -0.1142, -0.0331,  0.1372, -0.1167, -0.1371,\n",
       "                        0.1438, -0.0618, -0.2210, -0.0483,  0.0356,  0.0835, -0.1710, -0.1157,\n",
       "                       -0.0074, -0.0016, -0.0915,  0.0250],\n",
       "                      [ 0.0668, -0.1925, -0.2236,  0.0358,  0.0248,  0.1855, -0.1504,  0.0093,\n",
       "                       -0.0066,  0.0407, -0.1287,  0.0271,  0.0813, -0.2181,  0.0193, -0.2208,\n",
       "                       -0.1587, -0.0435,  0.1816,  0.1869],\n",
       "                      [ 0.1511,  0.0623, -0.1196, -0.1361, -0.1696,  0.1281, -0.0413, -0.0772,\n",
       "                        0.1233, -0.1363, -0.0965,  0.0523,  0.1157, -0.2202,  0.0786, -0.0674,\n",
       "                       -0.0084,  0.0508, -0.1838,  0.0230],\n",
       "                      [-0.1881,  0.0388, -0.1737, -0.2106,  0.0897, -0.1952,  0.1581,  0.0075,\n",
       "                        0.2059, -0.0216,  0.0880,  0.0200,  0.2084, -0.1206,  0.0182, -0.1588,\n",
       "                        0.1050,  0.2163, -0.1353, -0.1956],\n",
       "                      [ 0.0676, -0.0164,  0.0405, -0.0309,  0.1002, -0.0415, -0.0011, -0.0826,\n",
       "                        0.0840,  0.0618,  0.1796,  0.1354,  0.0401, -0.0202, -0.0193, -0.1139,\n",
       "                        0.1752, -0.1507,  0.2112, -0.0961],\n",
       "                      [-0.1230,  0.0041, -0.0572, -0.1598, -0.0510,  0.1162,  0.0054,  0.2181,\n",
       "                        0.0595, -0.1402, -0.1638, -0.1078, -0.1409,  0.0597, -0.0606, -0.1599,\n",
       "                        0.1455, -0.1982, -0.0924,  0.1143],\n",
       "                      [ 0.0866,  0.1306,  0.0380, -0.1974,  0.0584, -0.1985, -0.2227,  0.2175,\n",
       "                        0.0177, -0.1424,  0.2195, -0.0934, -0.1786,  0.1036,  0.1866, -0.1057,\n",
       "                        0.1528, -0.1007, -0.0477,  0.1327],\n",
       "                      [ 0.0877,  0.1709, -0.0735,  0.1271,  0.1377, -0.0919, -0.2072,  0.0187,\n",
       "                        0.0807, -0.1884, -0.1896,  0.1781,  0.0081, -0.1510,  0.2228,  0.1930,\n",
       "                       -0.0868,  0.0121, -0.1221,  0.0681],\n",
       "                      [ 0.1234,  0.1590,  0.2042,  0.0337, -0.1647, -0.2141,  0.0445,  0.1022,\n",
       "                       -0.1331,  0.0150, -0.1948,  0.0749,  0.0740, -0.2184, -0.0594, -0.1887,\n",
       "                       -0.1552, -0.1355,  0.1485, -0.1024],\n",
       "                      [-0.0135,  0.2093, -0.1994,  0.1120, -0.0217,  0.0059,  0.1582, -0.2106,\n",
       "                        0.0872,  0.1121, -0.0390,  0.0297,  0.1576, -0.1504,  0.1876, -0.1543,\n",
       "                        0.1166,  0.1807,  0.0718, -0.1915],\n",
       "                      [ 0.2218,  0.0501, -0.0653,  0.1415,  0.1528,  0.0723, -0.1763,  0.0403,\n",
       "                       -0.1047, -0.1264,  0.2068, -0.1246, -0.1587, -0.0711,  0.2207, -0.0749,\n",
       "                       -0.1744, -0.1212, -0.1296,  0.0868],\n",
       "                      [ 0.0974,  0.1246,  0.0074, -0.1825, -0.1509,  0.1098, -0.2059,  0.0424,\n",
       "                       -0.1496, -0.0257, -0.0727,  0.1273,  0.0286, -0.1648, -0.1466, -0.0179,\n",
       "                        0.0956, -0.0573, -0.0932, -0.1903],\n",
       "                      [ 0.0613,  0.1543,  0.0007,  0.0102, -0.1601, -0.1302, -0.1887, -0.0125,\n",
       "                       -0.1779, -0.1135,  0.0549, -0.1480,  0.1208,  0.1738,  0.0862,  0.1199,\n",
       "                       -0.1429,  0.0967,  0.0318, -0.0358],\n",
       "                      [-0.0251, -0.1228,  0.0220, -0.2083, -0.1426,  0.0652,  0.0480, -0.0167,\n",
       "                       -0.2155, -0.1523, -0.1959,  0.2095, -0.1649,  0.1739,  0.0561,  0.0246,\n",
       "                        0.0651, -0.1597, -0.0882,  0.1179],\n",
       "                      [-0.0634, -0.1821,  0.1044, -0.0956, -0.1416,  0.0959,  0.1301,  0.1276,\n",
       "                        0.1733,  0.0885, -0.1492,  0.1011, -0.1017,  0.0113,  0.1438, -0.1308,\n",
       "                       -0.1777, -0.2028,  0.0363,  0.0867],\n",
       "                      [ 0.1594,  0.2071,  0.0668,  0.2016, -0.0385,  0.1318, -0.1602, -0.1425,\n",
       "                        0.0826,  0.1328,  0.0336,  0.0608,  0.1494,  0.1309,  0.1112,  0.0641,\n",
       "                       -0.1883,  0.0374,  0.0536,  0.1485],\n",
       "                      [ 0.0485, -0.1009,  0.0851, -0.0898, -0.1109,  0.2058, -0.1559,  0.2120,\n",
       "                       -0.0585, -0.0721,  0.2089,  0.0369,  0.1929,  0.2133,  0.0379, -0.1901,\n",
       "                       -0.1712, -0.0282, -0.0987,  0.0252],\n",
       "                      [ 0.1972, -0.0705, -0.1602,  0.1540, -0.1903, -0.1969, -0.1195,  0.1940,\n",
       "                       -0.0794,  0.2123, -0.0106, -0.0117,  0.0076, -0.0576, -0.1499, -0.0231,\n",
       "                        0.0636,  0.0800, -0.0017,  0.0494],\n",
       "                      [ 0.1813,  0.0629, -0.1568, -0.0067, -0.0455,  0.0136, -0.1518, -0.0468,\n",
       "                        0.0786,  0.0370,  0.2234,  0.1294, -0.0320,  0.0456,  0.1774,  0.0749,\n",
       "                        0.2147,  0.2204, -0.1315, -0.1535],\n",
       "                      [ 0.2020, -0.1903,  0.1713, -0.2058, -0.0211,  0.0809, -0.1747,  0.1166,\n",
       "                        0.0810, -0.1743, -0.1918,  0.0211, -0.1711, -0.0918, -0.0764,  0.1157,\n",
       "                       -0.1129, -0.1312,  0.0720, -0.1918],\n",
       "                      [ 0.1397, -0.0116,  0.0276,  0.0743, -0.0852,  0.0377,  0.0947,  0.1684,\n",
       "                       -0.0028,  0.1058, -0.2191,  0.0712,  0.1310, -0.1188,  0.1824,  0.2163,\n",
       "                       -0.2046,  0.1543,  0.1575, -0.1961],\n",
       "                      [-0.0275, -0.1344, -0.2005, -0.1151, -0.1075, -0.0759,  0.1407,  0.1231,\n",
       "                       -0.1087, -0.1483,  0.0170, -0.1040, -0.0814,  0.0417, -0.1277,  0.0650,\n",
       "                        0.0685, -0.1472, -0.0420,  0.1628],\n",
       "                      [ 0.1512,  0.2214,  0.0421,  0.0056,  0.2162, -0.1399, -0.2225,  0.1628,\n",
       "                       -0.1357,  0.0151,  0.1517, -0.0996,  0.0373, -0.0698,  0.0657,  0.1399,\n",
       "                       -0.0194, -0.0927,  0.1473, -0.0476],\n",
       "                      [-0.0398,  0.2100,  0.1112,  0.1347,  0.0275, -0.1726, -0.1039,  0.0395,\n",
       "                        0.2154, -0.1850, -0.1470, -0.0128, -0.0798, -0.2207,  0.0917,  0.0836,\n",
       "                       -0.0566, -0.1213,  0.0796, -0.0903],\n",
       "                      [ 0.1454,  0.2195, -0.1113, -0.0895, -0.1711, -0.0940, -0.0119, -0.1862,\n",
       "                        0.1409, -0.2165,  0.0196,  0.1250,  0.0479, -0.1516,  0.0695, -0.1558,\n",
       "                       -0.1131,  0.1336, -0.0152, -0.0366],\n",
       "                      [ 0.1551,  0.1261,  0.1052, -0.1318,  0.1785,  0.1846, -0.0601,  0.1947,\n",
       "                        0.0392, -0.2231,  0.0258, -0.2122,  0.0806,  0.1925, -0.1680,  0.1205,\n",
       "                        0.2109, -0.1859, -0.0776,  0.1897],\n",
       "                      [ 0.0829,  0.0772, -0.0587,  0.1675,  0.0395, -0.1232, -0.1238, -0.2102,\n",
       "                       -0.1535,  0.0318,  0.0698, -0.0621, -0.2161, -0.0560,  0.1387,  0.0080,\n",
       "                       -0.0829,  0.1751,  0.1708,  0.1062],\n",
       "                      [-0.1380, -0.0542,  0.1221,  0.2004,  0.1339, -0.1739, -0.2158,  0.0620,\n",
       "                       -0.0438, -0.0148,  0.2182,  0.0343, -0.1972, -0.0239, -0.1197,  0.0637,\n",
       "                       -0.0302,  0.0949, -0.1911, -0.0003]])),\n",
       "             ('lin1.bias',\n",
       "              tensor([ 0.1028,  0.1930, -0.1998, -0.0867, -0.0466, -0.2039, -0.1836, -0.1828,\n",
       "                       0.0009, -0.0198, -0.0840, -0.1358, -0.2117, -0.0324, -0.1710,  0.1776,\n",
       "                      -0.1362, -0.1285, -0.1604, -0.1899,  0.2187,  0.0744,  0.1028,  0.0349,\n",
       "                      -0.0326,  0.1062, -0.1962, -0.1200, -0.0859, -0.0932])),\n",
       "             ('lin2.weight',\n",
       "              tensor([[ 0.0515, -0.0620,  0.1299,  ..., -0.0672,  0.1045, -0.1527],\n",
       "                      [ 0.0265,  0.1465, -0.1637,  ..., -0.1119,  0.1478,  0.0198],\n",
       "                      [ 0.1018, -0.0857, -0.0917,  ..., -0.0636,  0.0761, -0.0396],\n",
       "                      ...,\n",
       "                      [ 0.1478, -0.0057,  0.0197,  ..., -0.1485, -0.0839,  0.1667],\n",
       "                      [ 0.0418,  0.0486,  0.1381,  ..., -0.1380, -0.1543,  0.1205],\n",
       "                      [ 0.0824,  0.1378, -0.0323,  ..., -0.0072,  0.1052, -0.1528]])),\n",
       "             ('lin2.bias',\n",
       "              tensor([-0.1183, -0.0830, -0.1572, -0.0987,  0.1218,  0.0459, -0.1177, -0.1644,\n",
       "                       0.1197, -0.1800,  0.1132,  0.1253, -0.1442,  0.0487, -0.0613, -0.0550,\n",
       "                       0.0287, -0.1662, -0.1322,  0.0636, -0.0451, -0.1403, -0.0054,  0.0029,\n",
       "                       0.0695,  0.1082, -0.1444, -0.0684,  0.1753, -0.1492,  0.1697, -0.1596,\n",
       "                       0.1219,  0.0207, -0.0729, -0.0442,  0.0631, -0.0093,  0.0798,  0.1718])),\n",
       "             ('lin3.weight',\n",
       "              tensor([[-0.1184, -0.0344, -0.1081, -0.0766,  0.0338, -0.0215,  0.0054, -0.0515,\n",
       "                       -0.0143,  0.0762,  0.0351,  0.0718, -0.0921,  0.0438, -0.1042,  0.0281,\n",
       "                       -0.0512, -0.0605, -0.0157,  0.0665,  0.1571, -0.0694, -0.1532, -0.0666,\n",
       "                        0.0620,  0.1036,  0.1061, -0.0202,  0.0213,  0.1340, -0.0927,  0.0481,\n",
       "                       -0.0807, -0.0199, -0.0068,  0.0253,  0.1035, -0.0668, -0.0231, -0.0589],\n",
       "                      [-0.0229,  0.1206,  0.1563,  0.0366, -0.0513,  0.0763, -0.0961, -0.0994,\n",
       "                        0.1002, -0.0396, -0.1578, -0.1028, -0.0843,  0.1553,  0.0285, -0.0620,\n",
       "                        0.0394,  0.0698,  0.0256,  0.0203, -0.0452,  0.1448,  0.1411, -0.1017,\n",
       "                        0.0600,  0.1257, -0.1326, -0.0656, -0.1162,  0.1097,  0.1564, -0.0373,\n",
       "                       -0.1171,  0.0910,  0.0058,  0.0081, -0.0773, -0.0577,  0.1146,  0.0572],\n",
       "                      [-0.0349,  0.0031,  0.0306, -0.0378,  0.1217,  0.0880, -0.1263,  0.0759,\n",
       "                        0.0879, -0.1213,  0.0010,  0.0378, -0.0187,  0.0212,  0.0420, -0.1376,\n",
       "                       -0.1180,  0.0613,  0.0865, -0.1147,  0.1092, -0.0269,  0.1071, -0.0518,\n",
       "                        0.0497,  0.1198,  0.0818, -0.1503, -0.0381, -0.1337,  0.0333,  0.0019,\n",
       "                        0.0656, -0.0570, -0.0217,  0.1394,  0.0709, -0.0201, -0.0262, -0.0623]])),\n",
       "             ('lin3.bias', tensor([ 0.1379, -0.0143,  0.0128]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict() example\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c586f6",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "<div style=\"text-align:center\"> <img src='https://drive.google.com/thumbnail?id=1FRhniwGeeutBSJQRdW6GzshMfDrPz7oJ&sz=w1000' width=\"250\" height=\"200\"> </div>\n",
    "    \n",
    "Build a Fully connected neural network with\n",
    "\n",
    "- 3 layers\n",
    "- 마지막 layer의 unit 수는 `1` \n",
    "  - 마지막 layer의 activation은 없음 (linear layer)\n",
    "- Data feature 수는 `100`\n",
    "\n",
    "- input unit 수는 data 크기를 보고 맞추세요\n",
    "- hidden layer의 unit 수는 `[80, 50]`\n",
    "  - hidden layer의 activation 함수는 ReLU\n",
    "\n",
    "- model class 명 `myFCN`\n",
    "  - instance 명 `my_model` 생성\n",
    "  - `my_model` 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8150a-5d25-40fe-951a-416a5f022112",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "problem setup에서 구성한 neural network을 `nn.Sequential`을 활용하여 생성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3dabeb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 사용할 data \n",
    "batch_size = 30\n",
    "num_feature = 100\n",
    "\n",
    "X_train = torch.randn(batch_size, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dc5c673c-068b-4201-8900-3669a84d420a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "82fc8c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Problem 1 코딩 (매 줄마다 주석 필수 )\n",
    "\n",
    "# 클래스로 선언\n",
    "\n",
    "'''\n",
    "class myFCN(nn.Module):#nn.Module 써야 함. => Base class for all NN modules.\n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()#안쓰면 오류남. python문법 상속 클래스의 인스턴스를 임시로 불러옴, 그리고 실행.\n",
    "        \n",
    "        \n",
    "        self.lin1 = nn.Linear(100,80) #Input Hidden ,Data feature 수는 100\n",
    "        self.lin2 = nn.Linear(80,50) #Hidden Hidden.. 80unit\n",
    "        self.lin3 = nn.Linear(50,1) #Hidden Output..  50unit, 마지막 layer의 unit 수는 1\n",
    "        self.relu = nn.ReLU(True) #inplace연산해서 메모리 절약, ReLU사용\n",
    "        \n",
    "    #forward의 장점 : model.함수명 안하고 그냥 model만 적어도 알아서 forward불러옴\n",
    "    def forward(self, x): #\n",
    "        x = self.lin1(x) #1번째 레이어 통과(In)\n",
    "        x = self.relu(x) #활성화 함수\n",
    "        x = self.lin2(x) #2번째 레이어 통솨\n",
    "        x = self.relu(x) #활성화 함수\n",
    "        x = self.lin3(x) #세번째 레이어 통과 (Out)\n",
    "        #마지막 layer의 activation은 없음 (linear layer)\n",
    "        return x #반환\n",
    "'''\n",
    "# nn.Sequential 사용    \n",
    "class myFCN(nn.Module):\n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #nn.Sequential 사용하여 선언\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100,80), #1번째 레이어 100 > 80\n",
    "            nn.ReLU(True),    #활성화 함수\n",
    "            nn.Linear(80,50),  #2번째 레이어 80 > 50\n",
    "            nn.ReLU(True),     ##활성화 함수\n",
    "            nn.Linear(50,1)    #3번째 레이어 50 > 1\n",
    "            #마지막 레이어는 활성화함수 없이\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x) #위 과정 리턴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "846477b3-37df-43a8-874c-f6c903e797c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_model = myFCN() #instance 명 my_model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a46397a2-bf43-43cd-9c2c-fb1143009f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myFCN(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=80, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=80, out_features=50, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model#myModel의 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1cf8531b-0533-4137-9ea4-89896cab9d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0328],\n",
       "        [-0.1639],\n",
       "        [-0.1395],\n",
       "        [-0.0641],\n",
       "        [-0.1286],\n",
       "        [ 0.0244],\n",
       "        [-0.1074],\n",
       "        [-0.0883],\n",
       "        [-0.0153],\n",
       "        [-0.0887],\n",
       "        [-0.1091],\n",
       "        [ 0.0849],\n",
       "        [-0.0519],\n",
       "        [-0.1736],\n",
       "        [-0.1231],\n",
       "        [-0.0475],\n",
       "        [-0.1774],\n",
       "        [-0.0641],\n",
       "        [-0.0450],\n",
       "        [-0.1203],\n",
       "        [-0.0275],\n",
       "        [-0.1464],\n",
       "        [-0.0737],\n",
       "        [-0.0183],\n",
       "        [-0.1168],\n",
       "        [-0.0458],\n",
       "        [-0.0825],\n",
       "        [-0.0201],\n",
       "        [-0.0674],\n",
       "        [ 0.0536]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc48cc",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "problem setup에서 구성한 neural network을 `OrderedDict`을 활용하여 생성하세요\n",
    "- 각 layer의 이름을 주고 생성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "50011c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 답작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b7a32dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nn.Sequential 과 OrderedDict 사용    \n",
    "class myFCN(nn.Module):\n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #nn.Sequential 사용하여 선언, OrderedDict 사용하여 단계별 명명.\n",
    "        self.fc = nn.Sequential(OrderedDict([\n",
    "            ('layer_01', nn.Linear(100,80)), #1번째 레이어 100 > 80 레이어 이름 : layer_01\n",
    "            ('ReLU_01', nn.ReLU(True)),    #활성화 함수             활성화함수 이름 : ReLU_01\n",
    "            ('layer_02', nn.Linear(80,50)),  #2번째 레이어 80 > 50  레이어 이름 : layer_02\n",
    "            ('ReLU_02',nn.ReLU(True)),     ##활성화 함수            활성화함수 이름 : ReLU_02\n",
    "            ('layer_03', nn.Linear(50,1))    #3번째 레이어 50 > 1   레이어 이름 : layer_03\n",
    "            #마지막 레이어는 활성화함수 없이\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x) #위 과정 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3659cdb-bbb3-4857-89dc-d8c571140a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_model = myFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5f43915d-2d1c-4910-a465-438c8f1f0989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myFCN(\n",
       "  (fc): Sequential(\n",
       "    (layer_01): Linear(in_features=100, out_features=80, bias=True)\n",
       "    (ReLU_01): ReLU(inplace=True)\n",
       "    (layer_02): Linear(in_features=80, out_features=50, bias=True)\n",
       "    (ReLU_02): ReLU(inplace=True)\n",
       "    (layer_03): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d1142620-0363-4142-aa8a-2177d19898be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0878],\n",
       "        [ 0.0639],\n",
       "        [ 0.1186],\n",
       "        [ 0.1359],\n",
       "        [ 0.0436],\n",
       "        [-0.0698],\n",
       "        [ 0.0350],\n",
       "        [-0.0652],\n",
       "        [ 0.1491],\n",
       "        [-0.0685],\n",
       "        [ 0.0476],\n",
       "        [ 0.0190],\n",
       "        [ 0.0474],\n",
       "        [-0.0610],\n",
       "        [ 0.1055],\n",
       "        [ 0.0520],\n",
       "        [ 0.0893],\n",
       "        [-0.0064],\n",
       "        [-0.0300],\n",
       "        [ 0.0254],\n",
       "        [-0.0448],\n",
       "        [-0.0139],\n",
       "        [ 0.0117],\n",
       "        [-0.0371],\n",
       "        [-0.0651],\n",
       "        [ 0.0353],\n",
       "        [ 0.0020],\n",
       "        [ 0.1054],\n",
       "        [-0.0132],\n",
       "        [ 0.0697]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b877c-b9b1-4194-95f8-ea91c89fbbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e48f0-b6d4-4f85-beb9-ef43f5dee66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
